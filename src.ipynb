{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import faiss \n",
    "import os\n",
    "import numpy as np\n",
    "from accelerate import init_empty_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bits and bytes not optimized on windows\n",
    "#import bitsandbytes as bnb\n",
    "# print(\"Bitsandbytes version:\", bnb.__version__)\n",
    "# print(\"CUDA Available:\", bnb.is_available())\n",
    "# from bitsandbytes.cuda_setup.main import get_compute_capabilities, get_cuda_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Cache directory:\", os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "# print(\"Cache directory:\", os.getenv(\"HF_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bnb.__version__)\n",
    "# print(torch.__version__)\n",
    "# print(torch.version.cuda)\n",
    "# print(\"Detected CUDA Version (bitsandbytes):\", get_cuda_version())\n",
    "# print(\"Compute Capabilities:\", get_compute_capabilities())\n",
    "#print(\"Bitsandbytes CUDA Version:\", bnb.nvidia_driver_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-bit quantization \n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_quantized_model=\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\llm_rag_env\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\achen\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [04:02<00:00, 121.22s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model supports quantization: False\n"
     ]
    }
   ],
   "source": [
    "#check if model supports quantization\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # Change this to your model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(\"Model supports quantization:\", hasattr(model, \"quantization_config\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Check if remote code is required\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(\"Model Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299b591dd8a546e7ada7ad9476e49cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"D:/LLM_Projects/offload\",\n",
    "    #quantization_config=bnb_config,\n",
    "    #trust_remote_code=True\n",
    "    \n",
    ")\n",
    "\n",
    "prompt=\"Explain what a model is\"\n",
    "\n",
    "inputs=tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs={key:value.to(device) for key, value in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    output=model.generate(**inputs, max_length=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Explain what a model is in the context of machine learning.\n",
      "A model in the context of machine learning is a mathematical or algorithmic representation of a system, process or relationship between variables that can be used to make predictions or decisions. It is created by training a machine learning algorithm on a dataset, which allows the algorithm to learn patterns and make inferences about new data. The model can then be used to make predictions or decisions based on new input data. The accuracy and effectiveness of a\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
